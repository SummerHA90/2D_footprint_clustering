{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f4875f-8522-4c02-8bfc-b4acfc2705c2",
   "metadata": {},
   "source": [
    "**Author:** Summer </br>\n",
    "**Task:** 2D footprint clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6c853-7c73-4770-8fea-6e830e2beebd",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "This exercise is to reproduce the methodologies in [Duong et al. (2023)](https://doi.org/10.1145/3615900.3628790). The paper discusses the potential of using the concepts of graph similarity and spectral clustering to cluster a set of building footprints based on similarities of their shape characteristics. The authors suggest that a geometric graph distance, rooted in Wasserstein distance from transportation theory, is capable of capturing the graph structure as well as the spatial properties of the geometric graph. The methods and algorithm were tested on dataset from Boston, focusing on two sites: South End and Roxbury area, where they mainly have *row buildings, standalone houses, larger complex buildings, as well as equal sized buildings of similar shapes*. The results were validated through a small user study, and also compared with an existing feature-based clustering method. Future work is required to further the stability of the proposed similarity measure and to confirm the findings with more extensive experiments. Below is the workflow of the proposed method (Figure 1).\n",
    "\n",
    "![alt text](./images/model_components.png)\n",
    "\n",
    "*Figure 1 Model components and the parameters on the left hand side (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7945139f-7026-40d2-a8ce-5cad8f814b26",
   "metadata": {},
   "source": [
    "**Important Concepts**\n",
    "\n",
    "- **Medial axis**: This is the symmetric, skeletal structure of a geometric shape, such as a building footprint. The medial axis of a polygon refers to the set of points inside the building footprint that are equidistant to at least two different points on the boundary of the polygon (Figure 2).\n",
    "\n",
    "\n",
    "<img src=\"./images/examples_medial_axis.png\" alt=\"alt text\" width=\"400\" height=\"300\"/> </br>\n",
    "*Figure 2 Examples of medial axis (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)).* \n",
    "  \n",
    "- **Medial axis transformation**: This is a technique that helps to reconstruct the original shape of polygon *P* by taking the union of all points centered at the medial axis. The points from the medial axis can be partitioned into a set of curves, following the structure of the shape and describe its symmetry. These curves and their endpoints are the *edges* and *vertices* of the Medial Axis Graph. </br>\n",
    "\n",
    "- **Weisfeiler-Lehman Label Propogation**: To enable the comparison between graphs, the *Weisfeiler-Lehman Label Propagation* is a way to generate node labels based on the neighborhood around each node in a graph. Each node is given an initial label, and in each iteration, the current label is updated by combining its own label with the labels of its neighboring node. The idea behind is that vertices with the same initial label and with similar neighborhood structure will receive similar labels, enabling the comparison of shapes structurally using a graph method.\n",
    "\n",
    "<img src=\"./images/Weisfeiler-LehmanLabelPropagation.png\" alt=\"alt text\" width=\"400\" height=\"300\"/> </br>\n",
    "*Figure 3 One iteration of the Weisfeiler-Lehman label propogation (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)).*\n",
    "\n",
    "- **Graph Wassertein Distance**: To compute Graph Wassertein Distance, two graphs, $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$, will be compared. Specifically, the vertices in $V_1$ will be compared with the vertices in $V_2$. Note that $E$ is the edge set. During this process, a uniform weight of $ \\frac{1}{|V_1|} $ is assigned to each vertex in  $V_1$, and $ \\frac{1}{|V_2|} $ to each vertex in  $V_2$. Next, compute the cost matrix between vertices in $V_1$ and $V_2$ based on graph embedding, which maintains the structure of a graph while learning low-dimensional representations of its vertices. This, then, allows the matching between nodes of the two graphs.\n",
    "\n",
    "<img src=\"./images/distance_computation.png\" alt=\"alt text\" width=\"400\" height=\"300\"/> </br>\n",
    "*Figure 4 The influence of the mass of the vertices in the optimal transport problem. Vertices with similar colors in ùê∫1,ùê∫2, respectively, also have a similar embedding (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)).*\n",
    "\n",
    "- **Spectral clustering**: Spectral clustering creates a similarity graph or a distance graph using the pairwise similarity ($ s_{ij} $) or distance ($ d_{ij} $) based on $k$-nearest-neighbor graph. An edge is added between vertices $u$ and $v$ if either is among the other's $k$ nearest neighbors, with edge weights defined by their similarity  $s(u, v).$ The clustering algorithm then partitions the graph into $k$ groups, where higher-weight (similar) edges will stay within groups and lower-weight (dissimilar) edges across different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4164e25-9c60-4dee-8a9c-e2c534b3f4db",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Here are the required libraries to be installed in the terminal:\n",
    "\n",
    "```python\n",
    "pip install geopandas shapely scipy matplotlib \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a5991-ad7f-4cd3-b13a-c6fc74e17f69",
   "metadata": {},
   "source": [
    "## PART I: Skeleton Graph Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12171564-de92-401e-8e3d-affb26fc372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 1: Medial Axis Approximation\n",
    "# import libraries\n",
    "# import geopandas as gpd\n",
    "# from shapely.geometry import Polygon, MultiPolygon, LineString, Point\n",
    "# from shapely.ops import unary_union\n",
    "# from scipy.spatial import Voronoi\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d487562-d916-4a34-bd01-6b5f9390e014",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define functions\n",
    "# def sample_boundary_points(polygon, spacing=1.0):\n",
    "#     \"\"\"Sample points along the boundary of a polygon at regular spacing.\"\"\"\n",
    "#     # gives the outer boundary as a LineString\n",
    "#     boundary = polygon.exterior \n",
    "#     # gets the perimeter of the polygon\n",
    "#     length = boundary.length \n",
    "#     num_points = max(int(length / spacing), 2)\n",
    "#     # return discrete points that approximate the building footprint shapes\n",
    "#     return [boundary.interpolate(i / num_points, normalized=True) for i in range(num_points)] \n",
    "\n",
    "# def get_voronoi_edges(points, boundary):\n",
    "#     \"\"\"Compute Voronoi diagram and clip it to the polygon boundary.\"\"\"\n",
    "#     # converts the list of point objects into an array of coords\n",
    "#     coords = np.array([[p.x, p.y] for p in points])\n",
    "#     vor = Voronoi(coords)\n",
    "#     lines = []\n",
    "#     # iterates through the ridge vertices \n",
    "#     for vpair in vor.ridge_vertices:\n",
    "#         if -1 in vpair:\n",
    "#             continue  # Skip infinite ridges\n",
    "#         p1 = vor.vertices[vpair[0]]\n",
    "#         p2 = vor.vertices[vpair[1]]\n",
    "#         line = LineString([p1, p2]) # converts each vertex pair into a LineString\n",
    "#         if boundary.contains(line): # keep only the lines that are fully within the shape/polygon\n",
    "#             lines.append(line)\n",
    "\n",
    "#     return lines\n",
    "\n",
    "# def process_footprints(shapefile_path, spacing=1.0):\n",
    "#     \"\"\"Reads a shapefile of building footprints and processes each footprint to compute its approximate medial axis\"\"\"\n",
    "#     gdf = gpd.read_file(shapefile_path)\n",
    "#     medial_axes = []\n",
    "\n",
    "#     for idx, row in gdf.iterrows():\n",
    "#         geom = row.geometry\n",
    "#         if geom is None or not isinstance(geom, (Polygon, MultiPolygon)):\n",
    "#             continue\n",
    "\n",
    "#         if isinstance(geom, MultiPolygon):\n",
    "#             geom = unary_union(geom)\n",
    "\n",
    "#         points = sample_boundary_points(geom, spacing=spacing)\n",
    "#         edges = get_voronoi_edges(points, geom)\n",
    "#         medial_axes.extend(edges)\n",
    "\n",
    "#     return medial_axes\n",
    "\n",
    "# def plot_medial_axis(buildings_path, medial_axes):\n",
    "#     gdf = gpd.read_file(buildings_path)\n",
    "#     ax = gdf.plot(edgecolor='black', facecolor='none', figsize=(20, 20))\n",
    "#     for line in medial_axes:\n",
    "#         x, y = line.xy\n",
    "#         plt.plot(x, y, color='red', linewidth=1)\n",
    "#     plt.title(\"Approximate Medial Axis from Voronoi Diagram\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279e465-2632-4a01-9017-3c13848b9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "# shapefile = \"./data/529E/529E.shp\"\n",
    "# medial_axis = process_footprints(shapefile, spacing=2.0)\n",
    "# plot_medial_axis(shapefile, medial_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474ffb0-9e60-4cb5-bbbe-39ae4dec87ab",
   "metadata": {},
   "source": [
    "## PART I: Skeleton Graph Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf93fe53-f406-49bb-b1a9-3074acc2e5de",
   "metadata": {},
   "source": [
    "**Question(s)**\n",
    "- Should we treat multi-part polygons as one polygon? As in do we disregard the interior lines?\n",
    "- How do we handle footprints with open-porches (dashed lines) vs footprints with enclosed-porches (solid lines)?\n",
    "- In the paper, building footprints of similar shapes have almost identical graph structures, except the one with an extension (on top) has an additional branch. The authors claimed that structures like this are mainly for doors or bay windows placement and they do not contribute to the fundamental changes to the general shapes of the building footprint. So, they reduced details like these since the medial axis is sensitive to small changes in the shape of the building footprint.\n",
    "\n",
    "<img src=\"./images/branch_reduction.png\" alt=\"alt text\" width=\"400\" height=\"300\"/> </br>\n",
    "*Figure 5 The effects of the small branches on the medial axis and the results of Algorithm 1 (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1065bf82-ba4d-4dbb-9b5c-57b6fdc63f29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "\n",
    "### **Step 1: Medial Axis Approximation (Approach 1 with [Douglas-Peucker Algorithm](https://rdp.readthedocs.io/en/stable/))**\n",
    "\n",
    "**1.1 Approximating points on the boundary of building footprints**\n",
    "\n",
    "First, create an approximation algorithm to place points on the boundary of the building footprints in a way that they have equal distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6314a8-47dd-41e1-890e-e5e702be035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be827c-6167-47d7-b95e-b91a6a24ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define approximation function to place points on the boundary of the building footprints such that they have equal distance\n",
    "def sample_points_by_spacing(shapefile_path, spacing=2.0):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_sampled_points = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "\n",
    "        if geom is None or not isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = unary_union(geom)\n",
    "\n",
    "        boundary = geom.exterior\n",
    "        n_points = max(2, round(boundary.length / spacing))\n",
    "        distances = np.linspace(0, boundary.length, n_points)\n",
    "        points = [boundary.interpolate(distance) for distance in distances]\n",
    "\n",
    "        all_sampled_points.extend(points)\n",
    "\n",
    "    return gdf, gpd.GeoDataFrame(geometry=all_sampled_points, crs=gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec32261c-037f-43ad-a407-c9503eaab9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "buildings_gdf, points_gdf = sample_points_by_spacing(\"./data/input/529E/529E.shp\", spacing=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc47a11-aa5a-4f0a-997a-ad907e121eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "buildings_gdf.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "points_gdf.plot(ax=ax, color='blue', markersize=5)\n",
    "plt.title(\"Sampled Points Along Building Boundaries (Spacing = 0.01)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1964d4-7a70-4200-af70-e4e1494a5e8e",
   "metadata": {},
   "source": [
    "**1.2 Computing the Voronoi diagram and Creating the Medial Axis Graph**\n",
    "\n",
    "For the points obtained from 1.1, the Voronoi diagram is computed and the medial axis is taken from the bisectors of the Voronoi diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a564cc-4e06-4cec-95ad-7af63ee04215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon, MultiPolygon, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25692c29-28cb-48e0-92f8-6cbb2c321c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute medial axis from Voronoi bisectors\n",
    "def compute_medial_axis(points_gdf, buildings_gdf):\n",
    "    coords = np.array([[p.x, p.y] for p in points_gdf.geometry])\n",
    "    vor = Voronoi(coords)\n",
    "    medial_edges = []\n",
    "    medial_points_set = set()\n",
    "\n",
    "    buildings_union = buildings_gdf.union_all()\n",
    "\n",
    "    for vpair in vor.ridge_vertices:\n",
    "        if -1 in vpair:  # skip infinite ridges\n",
    "            continue\n",
    "        p1 = vor.vertices[vpair[0]]\n",
    "        p2 = vor.vertices[vpair[1]]\n",
    "        line = LineString([p1, p2])\n",
    "\n",
    "        if buildings_union.contains(line):\n",
    "            medial_edges.append(line)\n",
    "            # Store both vertices as tuples (hashable)\n",
    "            medial_points_set.add(tuple(p1))\n",
    "            medial_points_set.add(tuple(p2))\n",
    "\n",
    "    # Convert medial points set to GeoDataFrame\n",
    "    from shapely.geometry import Point\n",
    "    medial_points = [Point(xy) for xy in medial_points_set]\n",
    "    medial_points_gdf = gpd.GeoDataFrame(geometry=medial_points, crs=buildings_gdf.crs)\n",
    "\n",
    "    return medial_edges, medial_points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fc30d-9e38-41ff-ac7b-3aaaa119b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084723bb-0629-44f3-9d38-69a6bfebf1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "# def plot_medial_axis_with_points(buildings_gdf, points_gdf, medial_edges, medial_points_gdf):\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     medial_gdf = gpd.GeoDataFrame(geometry=medial_edges, crs=buildings_gdf.crs)\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(20, 20))\n",
    "#     buildings_gdf.plot(ax=ax, edgecolor='black', facecolor='none', label='Buildings')\n",
    "#     points_gdf.plot(ax=ax, color='blue', markersize=2, label='Sampled Points')\n",
    "#     medial_gdf.plot(ax=ax, color='red', linewidth=1, label='Medial Axis (Voronoi)')\n",
    "#     medial_points_gdf.plot(ax=ax, color='green', markersize=2, label='Medial Points')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.title('Building Footprints, Sampled Points, Medial Axis and Medial Points')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c9ada-9842-4057-8c5f-ba48f17bfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "def plot_medial_axis_with_points(buildings_gdf, points_gdf, medial_edges, medial_points_gdf):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import geopandas as gpd\n",
    "\n",
    "    medial_gdf = gpd.GeoDataFrame(geometry=medial_edges, crs=buildings_gdf.crs)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(24, 24))\n",
    "    buildings_gdf.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "    points_gdf.plot(ax=ax, color='blue', markersize=0.05)\n",
    "    medial_gdf.plot(ax=ax, color='red', linewidth=0.5)\n",
    "    medial_points_gdf.plot(ax=ax, color='green', markersize=0.05)\n",
    "\n",
    "    # Create legend\n",
    "    building_patch = mpatches.Patch(facecolor='none', edgecolor='black', label='Buildings')\n",
    "    points_handle = mlines.Line2D([], [], color='blue', marker='o', linestyle='None', markersize=2, label='Sampled Points')\n",
    "    medial_edges_handle = mlines.Line2D([], [], color='red', linewidth=1, label='Medial Axis (Voronoi)')\n",
    "    medial_points_handle = mlines.Line2D([], [], color='green', marker='o', linestyle='None', markersize=2, label='Medial Points')\n",
    "\n",
    "    ax.legend(handles=[building_patch, points_handle, medial_edges_handle, medial_points_handle])\n",
    "\n",
    "    plt.title('Building Footprints, Sampled Points, Medial Axis and Medial Points')\n",
    "    #plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8bc15b-fa33-4da2-ba81-fcc8cbf9283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "medial_edges, medial_points_gdf = compute_medial_axis(points_gdf, buildings_gdf)\n",
    "#plot_medial_axis_with_points(buildings_gdf, points_gdf, medial_edges, medial_points_gdf)\n",
    "\n",
    "fig_medial_axis = plot_medial_axis_with_points(buildings_gdf, points_gdf, medial_edges, medial_points_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25c033-83e4-42e4-999d-306db93bb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_medial_axis.savefig(\"./data/output/medial_axis_after_0.01.png\", dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473d4765-35af-4966-b221-58a1e9e7965b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Step 1: Medial Axis Approximation (Approach 2 Points Reduction)**\n",
    "\n",
    "**1.1 Approximating points on the boundary of building footprints**\n",
    "\n",
    "First, sample only the vertices where an edge changes direction ‚Äî i.e., the actual corner points of the building footprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422cb03-372f-408c-abb0-3fde56684c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point, LineString\n",
    "from shapely.ops import unary_union\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d169a4-c7c9-49a3-afa5-f30271955729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placing points on the vertices where an edge changes direction\n",
    "def sample_points_at_vertices(shapefile_path):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_vertex_points = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geom = row.geometry\n",
    "\n",
    "        if geom is None or not isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            continue\n",
    "\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = unary_union(geom)\n",
    "\n",
    "        exterior_coords = list(geom.exterior.coords)\n",
    "        vertex_points = [Point(xy) for xy in exterior_coords[:-1]]  # exclude duplicate last point\n",
    "        all_vertex_points.extend(vertex_points)\n",
    "\n",
    "    return gdf, gpd.GeoDataFrame(geometry=all_vertex_points, crs=gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114afbe-cbc9-4f09-938f-65ffa53c3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "buildings_gdf, vertex_points_gdf = sample_points_at_vertices(\"./data/input/529E/529E.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded14ba3-88f4-43dd-8e6a-8f5b60427c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(24, 24))\n",
    "buildings_gdf.plot(ax=ax, edgecolor='black', facecolor='none')\n",
    "vertex_points_gdf.plot(ax=ax, color='red', markersize=3)\n",
    "plt.savefig(\"./data/output/vetices_reduced_points.png\", dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576c2958-2c73-4854-a39d-b4debfcd26ff",
   "metadata": {},
   "source": [
    "**1.2 Computing the Voronoi diagram and Creating the Medial Axis Graph**\n",
    "\n",
    "For the points obtained from 1.1, the Voronoi diagram is computed and the medial axis is taken from the bisectors of the Voronoi diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509628e5-ef33-499f-ba2e-a1635c775999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_medial_axis(vertex_points_gdf, buildings_gdf):\n",
    "    coords = np.array([[p.x, p.y] for p in vertex_points_gdf.geometry])\n",
    "    vor = Voronoi(coords)\n",
    "    medial_edges = []\n",
    "    medial_points_set = set()\n",
    "\n",
    "    buildings_union = buildings_gdf.union_all()\n",
    "\n",
    "    for vpair in vor.ridge_vertices:\n",
    "        if -1 in vpair:  # skip infinite ridges\n",
    "            continue\n",
    "        p1 = vor.vertices[vpair[0]]\n",
    "        p2 = vor.vertices[vpair[1]]\n",
    "        line = LineString([p1, p2])\n",
    "\n",
    "        if buildings_union.contains(line):\n",
    "            medial_edges.append(line)\n",
    "            # Store both vertices as tuples (hashable)\n",
    "            medial_points_set.add(tuple(p1))\n",
    "            medial_points_set.add(tuple(p2))\n",
    "\n",
    "    # Convert medial points set to GeoDataFrame\n",
    "    from shapely.geometry import Point\n",
    "    medial_points = [Point(xy) for xy in medial_points_set]\n",
    "    medial_points_gdf = gpd.GeoDataFrame(geometry=medial_points, crs=buildings_gdf.crs)\n",
    "\n",
    "    return medial_edges, medial_points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa36778-83fe-4e3e-9ec8-6bd6012dd2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "medial_edges, medial_points_gdf = compute_medial_axis(vertex_points_gdf, buildings_gdf)\n",
    "medial_edges_gdf = gpd.GeoDataFrame(geometry=medial_edges, crs=buildings_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b574c-e501-4a31-b019-d41146151e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "buildings_gdf.plot(ax=ax, color=\"lightgrey\", edgecolor=\"black\", linewidth=0.5) # base layer, building footprint\n",
    "vertex_points_gdf.plot(ax=ax, color=\"green\", markersize=1) # sampled vertex points\n",
    "medial_edges_gdf.plot(ax=ax, color=\"blue\", linewidth=0.5) # medial edges (skeleton) \n",
    "medial_points_gdf.plot(ax=ax, color=\"red\", markersize=1) # medial axis points\n",
    "\n",
    "ax.set_title(\"Medial Axis of Building Footprints\")\n",
    "ax.axis(\"equal\")\n",
    "plt.savefig(\"./data/output/medial_axis_reduced_points.png\", dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8e77c-f359-44e3-ae95-0763cafd07b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Step 1: Medial Axis Approximation (Approach 3 with Scikit-Skeletonize)**\n",
    "\n",
    "[Scikit-skeletonize](https://scikit-image.org/docs/0.25.x/auto_examples/edges/plot_skeleton.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd968d-933b-450b-915c-1260c737fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.morphology import skeletonize\n",
    "from shapely.geometry import LineString, Point\n",
    "from rasterio import features\n",
    "from skimage.measure import label, regionprops\n",
    "from shapely.affinity import translate\n",
    "import shapely\n",
    "import rasterio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43448f6-28aa-4e83-a383-1d0d8a314816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_to_skeleton(polygon, pixel_size=1):\n",
    "    # Get bounds\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    width = int((maxx - minx) / pixel_size) + 1\n",
    "    height = int((maxy - miny) / pixel_size) + 1\n",
    "\n",
    "    # Create transform\n",
    "    transform = rasterio.transform.from_origin(minx, maxy, pixel_size, pixel_size)\n",
    "\n",
    "    # Rasterize\n",
    "    mask = features.rasterize(\n",
    "        [(polygon, 1)],\n",
    "        out_shape=(height, width),\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        all_touched=True,\n",
    "        dtype=np.uint8\n",
    "    )\n",
    "\n",
    "    # Skeletonize\n",
    "    skeleton = skeletonize(mask)\n",
    "\n",
    "    # Convert skeleton pixels to LineStrings\n",
    "    lines = []\n",
    "    for y in range(skeleton.shape[0]):\n",
    "        for x in range(skeleton.shape[1]):\n",
    "            if skeleton[y, x]:\n",
    "                px = transform * (x + 0.5, y + 0.5)\n",
    "                lines.append(Point(px))\n",
    "\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df71c10-b3f7-49b0-a510-6f6de5118cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_gdf = gpd.read_file(\"./data/input/529E/529E.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7d41b-987c-4ce7-bd39-10d2e6aeadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all skeleton points\n",
    "all_skeleton_points = []\n",
    "\n",
    "for poly in buildings_gdf.geometry:\n",
    "    if poly is None or not poly.is_valid or poly.is_empty:\n",
    "        continue\n",
    "    if poly.geom_type == \"MultiPolygon\":\n",
    "        for part in poly.geoms:\n",
    "            if part is not None and part.is_valid and not part.is_empty:\n",
    "                all_skeleton_points.extend(polygon_to_skeleton(part))\n",
    "    else:\n",
    "        all_skeleton_points.extend(polygon_to_skeleton(poly))\n",
    "\n",
    "# Create a GeoDataFrame for skeleton points\n",
    "skeleton_gdf = gpd.GeoDataFrame(geometry=all_skeleton_points, crs=buildings_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75c9cb-97cc-414a-b416-dcf54b5f4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "buildings_gdf.plot(ax=ax, facecolor=\"lightgray\", edgecolor=\"black\", linewidth=0.5)\n",
    "skeleton_gdf.plot(ax=ax, color=\"red\", markersize=1)\n",
    "ax.set_title(\"Skeletonized Medial Axis of Buildings\")\n",
    "plt.axis(\"equal\")\n",
    "plt.savefig(\"./data/output/medial_axis_skeletonized.png\", dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594d75b-53f4-4aa9-a546-03c792990093",
   "metadata": {},
   "source": [
    "### **Step 1: Medial Axis Approximation (Approach 4 Point reduction and edge construction in step 1, scikit geometry in step 2)**\n",
    "\n",
    "[Scikit-geometry documentation](https://scikit-geometry.github.io/scikit-geometry/skeleton.html)</br>\n",
    "[Scikit-geometry GitHub](https://github.com/scikit-geometry/scikit-geometry)\n",
    "</br>\n",
    "\n",
    "\n",
    "**Library installation process**\n",
    "\n",
    "There were issues with installing `skgeom` using `pip install skgeom`. The solution I used was:\n",
    "1. Clone the repository of `scikit-geometry` using `git clone https://github.com/scikit-geometry/scikit-geometry.git`.\n",
    "2. Install dependencies:\n",
    "   - `sudo apt libcdal-dev`\n",
    "3. In the cloned repository, change the file named `setup.py`, refer to the changes [here](https://github.com/scikit-geometry/scikit-geometry/pull/116/files).\n",
    "4. Initially, there were issues running the function `sg.skeleton.create_interior_straight_skeleton()`, which is the function to create skeleton from polygon. To fix this, revert the commit [868ef29](https://github.com/scikit-geometry/scikit-geometry/issues/103).\n",
    "5. Build and install `scikit-geometry`:\n",
    "   - In the local repository folder, use the command: `pip install -e . -v`.\n",
    "  \n",
    "For more complete documentation on Skeleton, refer to [CGAL](https://doc.cgal.org/latest/Straight_skeleton_2/group__PkgStraightSkeleton2SkeletonFunctions.html#gae1eec4fe2422502d32906f11306a4979)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91276a5-3c3f-4a32-a2ad-59d874543f4a",
   "metadata": {},
   "source": [
    "**1.1 Approximating points on vertices of building footprints and constructing edges between vertices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359669a-65b0-47b5-a4e2-b71cc69d33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, MultiPolygon, Point, LineString\n",
    "from shapely.ops import unary_union\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from scipy.stats import wasserstein_distance\n",
    "import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fd1a1-db51-423c-b112-5d7a76a00f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placing points on the vertices where an edge changes direction, constructing edges between vertices\n",
    "def sample_points_at_vertices(shapefile_path):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_vertex_points = []\n",
    "    all_edges = []\n",
    "\n",
    "    for idx, row in gdf.iterrows():\n",
    "        geom = row.geometry # extracts the geometry (i.e., building footprint) from the row in the gdf\n",
    "\n",
    "        # skip any rows without geometry or with a geometry that is not a polygon or multipolygon, null geometries\n",
    "        if geom is None or not isinstance(geom, (Polygon, MultiPolygon)):\n",
    "            print(idx)\n",
    "            print(geom)\n",
    "            continue\n",
    "\n",
    "        # if the geometry is a multipolygon, merge them into a single polygon\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = unary_union(geom)\n",
    "\n",
    "        exterior_coords = list(geom.exterior.coords) # get the coordinates of the outer boundary of the polygon, in a sequence of (x,y) tuples\n",
    "        coords = exterior_coords[:-1]\n",
    "        vertex_points = [Point(xy) for xy in exterior_coords[:-1]]  # convert each (x,y) coordinate into a Point object, exclude the last coordinate since it duplicates the first (to close the polygon)\n",
    "        all_vertex_points.extend(vertex_points) # add all the Point objects of a polygon to the list all_vertex_points\n",
    "\n",
    "        # Create edges between consecutive vertices\n",
    "        edges = [LineString([coords[i], coords[i + 1]]) for i in range(len(coords) - 1)]\n",
    "        # Add edge between the last and first to close the polygon\n",
    "        edges.append(LineString([coords[-1], coords[0]]))\n",
    "\n",
    "        all_edges.extend(edges)\n",
    "\n",
    "    # Return GeoDataFrames for points and edges\n",
    "    points_gdf = gpd.GeoDataFrame(geometry=all_vertex_points, crs=gdf.crs)\n",
    "    edges_gdf = gpd.GeoDataFrame(geometry=all_edges, crs=gdf.crs)\n",
    "\n",
    "    return gdf, points_gdf, edges_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b5413-7de1-4133-9742-2027ed24046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use case\n",
    "buildings_gdf, points_gdf, edges_gdf = sample_points_at_vertices(\"./data/input/529E/529E.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009be4ff-e431-4556-a21d-f2b0d4cf742b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(24, 24))\n",
    "edges_gdf.plot(ax=ax, edgecolor='blue', linewidth=1, facecolor='none')\n",
    "points_gdf.plot(ax=ax, color='red', markersize=3)\n",
    "plt.savefig(\"./data/output/vetices_edges_reduced_points.png\", dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde376aa-20f5-40b0-87a7-2ba8be0e87d2",
   "metadata": {},
   "source": [
    "**1.2 Medial axis construction using scikit-geometry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceca071-6966-42db-877c-0ec238f34c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skgeom as sg\n",
    "from skgeom.draw import draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e1a9b-055c-4d83-91a0-94173ab917c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buildings_gdf \n",
    "# buildings_gdf.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746e1e49-30f0-4918-8d41-367e07d0ba2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(buildings_gdf.geometry[0].boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796c514-2c9d-4d23-9257-8a0cf7b60693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_coord = buildings_gdf.geometry[0].boundary.coords[0][0]\n",
    "# y_coord = buildings_gdf.geometry[0].boundary.coords[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65556c1-e7a5-4ab9-a847-889342198be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract points from one polygon\n",
    "# points = []\n",
    "\n",
    "# for x,y in buildings_gdf.geometry[4].boundary.coords[:-1]:\n",
    "#     #print(x, y)\n",
    "#     # points.append([x_coords,y_coords])\n",
    "#     points.append(sg.Point2(x,y))\n",
    "# print(points[::-1]) # make sure it is counter-clockwise\n",
    "\n",
    "# # construct a polygon using points\n",
    "# poly = sg.Polygon(points[::-1])\n",
    "# draw(poly)\n",
    "\n",
    "# # construct skeleton from polygon\n",
    "# skel = sg.skeleton.create_interior_straight_skeleton(poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eacd8e-d77f-48c6-a4ed-581c4e7f09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(skel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c662-c4b8-4c83-b7d0-3e92a7eca851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton(polygon, skeleton, show_time=False):\n",
    "    draw(polygon)\n",
    "\n",
    "    for h in skeleton.halfedges:\n",
    "        if h.is_bisector:\n",
    "            p1 = h.vertex.point\n",
    "            p2 = h.opposite.vertex.point\n",
    "            plt.plot([p1.x(), p2.x()], [p1.y(), p2.y()], 'r-', lw=2)\n",
    "\n",
    "    if show_time:\n",
    "        for v in skeleton.vertices:\n",
    "            plt.gcf().gca().add_artist(plt.Circle(\n",
    "                (v.point.x(), v.point.y()),\n",
    "                v.time, color='blue', fill=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36552932-d745-4dbb-82a6-0b98af74c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_skeleton(poly, skel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5619a-9ed9-4e23-8521-214f5999da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(buildings_gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215856e-93dd-4170-b9b6-8e01dfbd0661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_poly = []\n",
    "all_skeleton = []\n",
    "\n",
    "for idx, row in buildings_gdf.iterrows():\n",
    "    geom = row.geometry\n",
    "    #print(geom)\n",
    "\n",
    "    # skip any rows without geometry or with a geometry that is not a polygon or multipolygon, null geometries\n",
    "    if geom is None or not isinstance(geom, (Polygon, MultiPolygon)): \n",
    "        print(idx)\n",
    "        print(geom)\n",
    "        continue\n",
    "\n",
    "    # continue to work with geom that is not null\n",
    "    # for each geom, extract the points and convert them to sg.Point2; use the points to create and draw poly; create the skeleton for each geom\n",
    "    points = []\n",
    "    for x,y in geom.boundary.coords[:-1]: # [:-1] to exclude the last point that is used to close the polygon\n",
    "        points.append(sg.Point2(x,y))\n",
    "         \n",
    "    poly = sg.Polygon(points[::-1]) # reverse order of the points    \n",
    "    all_poly.append(poly)\n",
    "    skel = sg.skeleton.create_interior_straight_skeleton(poly)        \n",
    "    all_skeleton.append(skel)\n",
    "\n",
    "#print(idx)\n",
    "#print(all_poly)\n",
    "#len(all_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a0574-92f7-4241-8603-5b07ab63899b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24,24))\n",
    "draw(all_poly, plt=plt, line_width=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd214f-ac75-42b4-a729-e3091ec6be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_all_skeletons(polygon, all_skeletons, show_time=False, figsize=(24,24)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    draw(polygon, line_width=0.5)\n",
    "    \n",
    "    for item in all_skeletons:\n",
    "\n",
    "        if item is None:\n",
    "            continue\n",
    "        \n",
    "        for h in item.halfedges:\n",
    "            if h.is_bisector:\n",
    "                p1 = h.vertex.point\n",
    "                p2 = h.opposite.vertex.point\n",
    "                plt.plot([p1.x(), p2.x()], [p1.y(), p2.y()], 'r-', lw=0.5)\n",
    "\n",
    "        if show_time:\n",
    "            for v in skeleton.vertices:\n",
    "                plt.gcf().gca().add_artist(plt.Circle(\n",
    "                    (v.point.x(), v.point.y()),\n",
    "                    v.time, color='blue', fill=False))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f941f-6b51-42a9-aa51-64ce452554b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, x in enumerate(all_skeleton):\n",
    "    if x == None:\n",
    "        print(idx, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ff961-0c23-4c3a-b6b2-4eb48755de2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_all_skeletons(all_poly,all_skeleton)\n",
    "plt.savefig(\"./data/output/scikit_geom.png\", dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9984eb-e060-4cf0-a00d-f4d7a7026c15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Part of Part I in the paper (maybe not needed in our approach 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e649e-7b71-4deb-831c-e742515a5f26",
   "metadata": {},
   "source": [
    "**1.3 Path Decomposition**\n",
    "\n",
    "The medial axis of a polygon (i.e., building footprint) is a planar graph whose vertices can be endpoints (degree¬†1), junctions (degree¬†‚â•3), or simple chain points (degree¬†2). This step performs path decomposition of the medial axis graph into sub-paths where internal vertices have degree 2. Each sub-path corresponds to a curve that can be simplified, allowing for point reduction in the next step.\n",
    "\n",
    "Let $W = (v_0, ..., v_m)$ be a sequence of vertices of path $W$ in $G$ such that:\n",
    "- deg($v_0$) $\\neq$ 2,\n",
    "- deg($v_m$) $\\neq$ 2, and\n",
    "- deg($v_1$) = ... = deg($v_{m-1}$) = 2\n",
    "\n",
    "Given that the path decomposition of $G$, each of these paths corresponds to a curve that can be simplified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c81305-5195-4e91-b758-4fcf101fa58f",
   "metadata": {},
   "source": [
    "**1.4 Point Reduction using [Douglas-Peucker Algorithm](https://rdp.readthedocs.io/en/stable/)**\n",
    "\n",
    "Apply the [Douglas-Peucker Algorithm](https://rdp.readthedocs.io/en/stable/) to each path to simplify the curves by reducing points but preserving the overall shape. The path decomposition ensures that the graph topology is not changed with respect to the characteristic points of the medial axis representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7040c83-d886-4dab-9fae-c2381274f042",
   "metadata": {},
   "source": [
    "**1.5 Branch Reduction**\n",
    "\n",
    "This step removes some \"unnecessary\" details or extensions of building footprints, so that buildings with similar shapes but small variations will still be able to generate identical medial graph. An algoritm developed by the authors helps to identify small brances and decides whether to remove them or not.\n",
    "\n",
    "**Algorithm 1**: Branch Reduction Algorithm (Source: [Duong et al., 2023](https://doi.org/10.1145/3615900.3628790)). </br>\n",
    "<img src=\"./images/branch_reduction_algo.png\" alt=\"alt text\" width=\"400\" height=\"700\"/> </br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16f283-f3fa-48b8-9d41-d155961a529f",
   "metadata": {},
   "source": [
    "## PART II: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d6aa3e-4405-4e3d-99ea-aacd0d43d439",
   "metadata": {},
   "source": [
    "**2.1 Graph Embedding and Weisfeiler-Lehman Label Propagation** </br>\n",
    "*Ningchuan: can use networkX python library*\n",
    "\n",
    "This step aims to generate node labels based on the neighborhood around each node in a graph. Each node is given an initial label, and in each iteration, the current label is updated by combining its own label with the labels of its neighboring node. The idea behind is that vertices with the same initial label and with similar neighborhood structure will receive similar labels, enabling the comparison of shapes structurally using a graph method.\n",
    "\n",
    "***From the paper:*** Constructed from the medial axis graph, the graph for a polygon (or a building footprint) is represented as $G = (V, E)$. The graphs are undirected and the vertex attribute $a(v)$ denotes the radius of the corresponding vertex of the medial axis transform for all $v \\in V$. \n",
    "\n",
    "***STEP 1: Approach attempted for creating networkx graphs:*** \n",
    "This step aims to translate a skeleton graph into a networkx graph for each building footprint. Refer to [networkx documentation](https://networkx.org/documentation/networkx-1.7/tutorial/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f42ac3-634e-4481-81f7-58a45e39d21c",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Trying with individual building footprints\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a274c-b206-4474-858e-6392ed4258b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing idea for 1 polygon only\n",
    "# extract points from one polygon\n",
    "points1 = []\n",
    "\n",
    "for x,y in buildings_gdf.geometry[1].boundary.coords[:-1]:\n",
    "    #print(x, y)\n",
    "    # points1.append([x_coords,y_coords])\n",
    "    points1.append(sg.Point2(x,y))\n",
    "print(points1[::-1]) # make sure it is counter-clockwise\n",
    "\n",
    "# construct a polygon using points\n",
    "poly1 = sg.Polygon(points1[::-1])\n",
    "#draw(poly1)\n",
    "\n",
    "# construct skeleton from polygon\n",
    "skel1 = sg.skeleton.create_interior_straight_skeleton(poly1)\n",
    "#skel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdda030-77e8-485d-8cbb-75b16f7a96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract points from second polygon\n",
    "points2 = []\n",
    "\n",
    "for x,y in buildings_gdf.geometry[2].boundary.coords[:-1]:\n",
    "    #print(x, y)\n",
    "    # points2.append([x_coords,y_coords])\n",
    "    points2.append(sg.Point2(x,y))\n",
    "print(points2[::-1]) # make sure it is counter-clockwise\n",
    "\n",
    "# construct a polygon using points\n",
    "poly2 = sg.Polygon(points2[::-1])\n",
    "#draw(poly2)\n",
    "\n",
    "# construct skeleton from polygon\n",
    "skel2 = sg.skeleton.create_interior_straight_skeleton(poly2)\n",
    "#skel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc05908-6390-4d3c-91b4-d8344c20b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract points from third polygon\n",
    "points3 = []\n",
    "\n",
    "for x,y in buildings_gdf.geometry[3].boundary.coords[:-1]:\n",
    "    #print(x, y)\n",
    "    # points3.append([x_coords,y_coords])\n",
    "    points3.append(sg.Point2(x,y))\n",
    "print(points3[::-1]) # make sure it is counter-clockwise\n",
    "\n",
    "# construct a polygon using points\n",
    "poly3 = sg.Polygon(points3[::-1])\n",
    "#draw(poly2)\n",
    "\n",
    "# construct skeleton from polygon\n",
    "skel3 = sg.skeleton.create_interior_straight_skeleton(poly3)\n",
    "#skel3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b42910-44ea-4178-ba45-6aef143a2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton_only(polygon, skeleton, ax, show_time=False):\n",
    "    #draw(polygon)\n",
    "\n",
    "    for h in skeleton.halfedges:\n",
    "        if h.is_bisector:\n",
    "            p1 = h.vertex.point\n",
    "            p2 = h.opposite.vertex.point\n",
    "            ### this is for single graph\n",
    "            #plt.plot([p1.x(), p2.x()], [p1.y(), p2.y()], 'r-', lw=2) \n",
    "            ### this is for single graph\n",
    "            ax.plot([p1.x(), p2.x()], [p1.y(), p2.y()], 'r-', lw=1.5)\n",
    "            \n",
    "    if show_time:\n",
    "        ### this is for single graph\n",
    "        # for v in skeleton.vertices: \n",
    "        #     plt.gcf().gca().add_artist(plt.Circle(\n",
    "        #         (v.point.x(), v.point.y()),\n",
    "        #         v.time, color='blue', fill=False))\n",
    "        ### this is for single graph\n",
    "        for v in skeleton.vertices:\n",
    "            ax.add_artist(plt.Circle(\n",
    "                (v.point.x(), v.point.y()),\n",
    "                v.time, color='blue', fill=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29a34b2-aec6-469f-a156-f150226a3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw 3 skeleton graphs\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))\n",
    "\n",
    "axs[0].set_title(\"Skeleton 1\")\n",
    "axs[0].set_frame_on(True)\n",
    "draw_skeleton_only(poly1, skel1, ax=axs[0])\n",
    "axs[0].axis('on')\n",
    "\n",
    "axs[1].set_title(\"Skeleton 2\")\n",
    "axs[1].set_frame_on(True)\n",
    "draw_skeleton_only(poly2, skel2, ax=axs[1])\n",
    "axs[1].axis('on')\n",
    "\n",
    "axs[2].set_title(\"Skeleton 3\")\n",
    "axs[2].set_frame_on(True)\n",
    "draw_skeleton_only(poly3, skel3, ax=axs[2])\n",
    "axs[2].axis('on')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a46092-f31e-4bd8-a342-2310009dcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vertices and edges from the above skeleton\n",
    "poly_vertices1 = list(skel1.vertices)\n",
    "poly_edges1 = list(skel1.halfedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecbb217-d357-4366-9633-848c6c7ffedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vertices and edges from the second skeleton\n",
    "poly_vertices2 = list(skel2.vertices)\n",
    "poly_edges2 = list(skel2.halfedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1426c-44be-4a33-a1d3-f1e6d29160a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vertices and edges from the third skeleton\n",
    "poly_vertices3 = list(skel3.vertices)\n",
    "poly_edges3 = list(skel3.halfedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad3aab-1a39-4401-a6ee-4cb3edd6b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "#poly_vertices1[2]\n",
    "#poly_edges1[0].vertex\n",
    "#poly_edges1[0].vertex in poly_vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a331c24-d075-4185-baf2-103670019cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(poly_edges1), len(poly_edges2), len(poly_edges3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd5cad1-7702-4cbb-8b1d-eef7df70f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a networkx graph from a skeleton graph\n",
    "#import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff93f83-6463-42c8-8e7a-5a81949d0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#G.add_edge(poly_edges[0].vertex,poly_edges[0].opposite.vertex)\n",
    "\n",
    "# create an empty graph with no nodes and no edges\n",
    "G1=nx.Graph()\n",
    "\n",
    "# grow undirected (since it adds up from both direction) graph G by adding edge, done by specifying the vertex id at two ends to determine the edge\n",
    "for item in poly_edges1:\n",
    "    if item.is_bisector:\n",
    "        p1 = item.vertex.id # vertex id\n",
    "        p2 = item.opposite.vertex.id\n",
    "        G1.add_edge(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3128118-0bc9-46a1-8ce0-0cfbbb2a014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty graph with no nodes and no edges\n",
    "G2=nx.Graph()\n",
    "\n",
    "# grow undirected (since it adds up from both direction) graph G by adding edge, done by specifying the vertex id at two ends to determine the edge\n",
    "for item in poly_edges2:\n",
    "    if item.is_bisector:\n",
    "        p1 = item.vertex.id # vertex id\n",
    "        p2 = item.opposite.vertex.id\n",
    "        G2.add_edge(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437305f9-dd81-4858-9a7c-f094746b0b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty graph with no nodes and no edges\n",
    "G3=nx.Graph()\n",
    "\n",
    "# grow undirected (since it adds up from both direction) graph G by adding edge, done by specifying the vertex id at two ends to determine the edge\n",
    "for item in poly_edges3:\n",
    "    if item.is_bisector:\n",
    "        p1 = item.vertex.id # vertex id\n",
    "        p2 = item.opposite.vertex.id\n",
    "        G3.add_edge(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce5e2a-737a-4923-a3e9-e24ee51f5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw graphs\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))  # 1 row, 3 columns\n",
    "\n",
    "# Plot G1\n",
    "axs[0].set_title(\"Graph G1\")\n",
    "axs[0].set_frame_on(True)  # ensure box is shown\n",
    "nx.draw(G1, with_labels=True, ax=axs[0])\n",
    "axs[0].axis('on')  # shows axis boundary (box)\n",
    "\n",
    "# Plot G2\n",
    "axs[1].set_title(\"Graph G2\")\n",
    "axs[1].set_frame_on(True)\n",
    "nx.draw(G2, with_labels=True, ax=axs[1])\n",
    "axs[1].axis('on')\n",
    "\n",
    "# Plot G3\n",
    "axs[2].set_title(\"Graph G3\")\n",
    "axs[2].set_frame_on(True)\n",
    "nx.draw(G3, with_labels=True, ax=axs[2])\n",
    "axs[2].axis('on')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4e9ff-65a1-44de-b3ce-764dce418e8a",
   "metadata": {},
   "source": [
    "***STEP 2: Weisfeiler-Lehman label propagation***\n",
    "\n",
    "***Attempted approach here:*** Used the `weisfeiler_lehman_subgraph_hashes()` function from `networkx`. Refer to documentation [here](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.graph_hashing.weisfeiler_lehman_subgraph_hashes.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899d70b-cf5b-4e19-ae2b-8a56a2825492",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Trying with individual building footprints\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eed9b5-c2bc-405f-a03f-d3003f01692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar nodes in different graphs\n",
    "g1_hashes = nx.weisfeiler_lehman_subgraph_hashes(\n",
    "    G1, iterations=4, digest_size=8\n",
    ")\n",
    "\n",
    "g2_hashes = nx.weisfeiler_lehman_subgraph_hashes(\n",
    "    G2, iterations=4, digest_size=8\n",
    ")\n",
    "\n",
    "g3_hashes = nx.weisfeiler_lehman_subgraph_hashes(\n",
    "    G3, iterations=4, digest_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffa875-0d19-430c-ba88-c9d9e6b32b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_hashes[1] # the hash sequence of depth 3 for node 1 in G1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92b63c9-2442-4aa7-9f35-e71b476c2dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_hashes[1] # the hash sequence of depth 3 for node 1 in G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7f7d2-f19c-4e98-8877-47235e39bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3_hashes[1] # the hash sequence of depth 3 for node 1 in G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719381b-9c4a-4971-8f8d-76421980ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_hashes == g2_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59944e2f-aaa3-48d8-a5a3-bcd57dfe5e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_hashes == g3_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79b1eb-ee2b-4b76-ac11-7b7af945c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2_hashes == g3_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937229c-bdc0-4a94-b060-1d77253aa8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get hash values for each node (latest iteration's hash)\n",
    "node_labels1 = {node: hash_list[-1] for node, hash_list in g1_hashes.items()}\n",
    "node_labels2 = {node: hash_list[-1] for node, hash_list in g2_hashes.items()}\n",
    "node_labels3 = {node: hash_list[-1] for node, hash_list in g3_hashes.items()}\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 6))  # 1 row, 3 columns\n",
    "\n",
    "# plot graphs with hash labels\n",
    "# G1\n",
    "pos1 = nx.spring_layout(G1, seed=42, scale=0.8)  # or any other layout\n",
    "axs[0].set_title(\"WL Subtree Hashes Graph 1 (Iteration 4)\")\n",
    "axs[0].set_frame_on(True)  # ensure box is shown\n",
    "nx.draw(G1, pos1, ax=axs[0], with_labels=False, node_color='lightblue', edge_color='gray', node_size=500)\n",
    "nx.draw_networkx_labels(G1, pos1, ax=axs[0], labels=node_labels1, font_size=8, font_family=\"monospace\")\n",
    "axs[0].axis('on')  # shows axis boundary (box)\n",
    "\n",
    "# G2\n",
    "pos2 = nx.spring_layout(G2, seed=42, scale=0.8)  # or any other layout\n",
    "axs[1].set_title(\"WL Subtree Hashes Graph 2 (Iteration 4)\")\n",
    "axs[1].set_frame_on(True)  # ensure box is shown\n",
    "nx.draw(G2, pos2, ax=axs[1], with_labels=False, node_color='lightblue', edge_color='gray', node_size=500)\n",
    "nx.draw_networkx_labels(G2, pos2, ax=axs[1], labels=node_labels2, font_size=8, font_family=\"monospace\")\n",
    "axs[1].axis('on')  # shows axis boundary (box)\n",
    "\n",
    "# G3\n",
    "pos3 = nx.spring_layout(G3, seed=42, scale=0.8)  # or any other layout\n",
    "axs[2].set_title(\"WL Subtree Hashes Graph 3 (Iteration 4)\")\n",
    "axs[2].set_frame_on(True)  # ensure box is shown\n",
    "nx.draw(G3, pos3, ax=axs[2], with_labels=False, node_color='lightblue', edge_color='gray', node_size=500)\n",
    "nx.draw_networkx_labels(G3, pos3, ax=axs[2], labels=node_labels3, font_size=8, font_family=\"monospace\")\n",
    "axs[2].axis('on')  # shows axis boundary (box)\n",
    "\n",
    "axs[0].margins(0.1)\n",
    "axs[1].margins(0.1)\n",
    "axs[2].margins(0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cceb1a7-c601-4e12-8076-3328c29678d9",
   "metadata": {},
   "source": [
    "***STEP 3: Graph Embedding***\n",
    "\n",
    "***From the paper:*** Each edge $e = {u, v} \\in E$ should be weighted by the Euclidean distance between the real-world coordinates of the endpoints $u$ and $v$. The authors changed the edge weights $w(e)$ from the Euclidean distance of the endpoints of the edge $e$, to the reciprocal weight $w'(e) = \\frac{1}{w(e)}$ to control the feature scaling throughout the iterations. Additionally, the neighborhood should be weighted by the sum of the edge weights instead of the degree. \n",
    "\n",
    "The graph embedding is obtained by taking the weighted vectors of the Weisfeiler-Lehman features of each iteration $i$, which is given by $X(G_i) = [a_i(v_1),...,a_i(v_n)]$.\n",
    "\n",
    "***Attempted approach here:*** We do not have the algorithm to weight the edge. Hence, we assume uniform weights for all edges. With the uniform weights, we proceed with creating node embeddings of graph $G$ at iteration $H$, [Togninalli et al. (2019)](https://arxiv.org/pdf/1906.01277)) defined it as:\n",
    "\n",
    "$f^H: G \\rightarrow \\mathbb{R}^{n_G} \\times (m(H+1))$ </br>\n",
    "$G \\mapsto \\text {concatenate}(X^0_G,\\dots,X^H_G)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651ad52-2c19-4e91-8abb-c2d55475b682",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Trying with individual building footprints\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660f030-7837-4982-ae4b-e0f89553a3af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create node embeddings of graph\n",
    "\n",
    "# G1\n",
    "G1_nf = [] # nf denotes node feature\n",
    "for i in g1_hashes.values():\n",
    "   G1_nf.append(i) \n",
    "\n",
    "# G2\n",
    "G2_nf = []\n",
    "for i in g2_hashes.values():\n",
    "   G2_nf.append(i) \n",
    "\n",
    "# G3\n",
    "G3_nf = []\n",
    "for i in g3_hashes.values():\n",
    "   G3_nf.append(i) \n",
    "\n",
    "print(f\"G1: {G1_nf}; \\nG2: {G2_nf}; \\nG3: {G3_nf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abde0d-41e6-4bce-b1b0-b534ad68f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of list to matrix for the subsequent computation\n",
    "# G1\n",
    "G1_nf_m = np.array(G1_nf)\n",
    "G1_nf_m.shape # 6x4\n",
    "\n",
    "# G2\n",
    "G2_nf_m = np.array(G2_nf)\n",
    "G2_nf_m.shape # 6x4\n",
    "\n",
    "# G3\n",
    "G3_nf_m = np.array(G3_nf)\n",
    "G3_nf_m.shape # 10x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c47b1-f901-415c-8e9e-189b1b15f8e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"G1 matrix:{G1_nf_m}; \\nG2 matrix:{G2_nf_m}; \\nG3 matrix:{G3_nf_m};\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa957fd-5906-47c4-aacc-6fbaa1135ca8",
   "metadata": {},
   "source": [
    "**2.2 Graph Wasserstein Distance using Optimal Transport Theory**\n",
    "\n",
    "***From the paper:*** The final distance between two skeleton graphs of building footprints is determined by solving the optimal transport problem between the embedded vertex sets of the two graphs. The authors note that it is also necessary to assign weights to the vertices. Previous scholars used uniform weights; however, the authors observe that this significantly influences the distance measure such that similar buildings do not necessarily have a smaller distance. This occurred because for buildings with similar shape, the resulting skeleton graphs might be different; different building sizes (skeleton graphs) may yield differences in terms of the radius of the leaf vertices and inner vertices. Hence, the vertices should be weighted by degree. \n",
    "\n",
    "The authors define the mass of a vertex $u \\in V$ by:\n",
    "\n",
    "$w(u) = \\frac{deg(u)}{deg_ {total}}$\n",
    "\n",
    "with \n",
    "\n",
    "$deg_{total} = \\Sigma_{u \\in V} deg(v)$\n",
    "\n",
    "With this, the inner vertices get a higher weight than the leaf vertices.\n",
    "\n",
    "***Attempted approach here:*** </br>\n",
    "*Graph Embedding Scheme*: Given a graph $G = (V,E)$, a graph embedding scheme is a function that outputs a fixed-size vectorial representation for each node in the graph.</br>\n",
    "*Graph Wasserstein Distance*: Given two graphs $G = (V,E)$ and $G' = (V',E')$, a graph embedding scheme, and a ground distance, [Togninalli et al. (2019)](https://arxiv.org/pdf/1906.01277) defined the Graph Wasserstein Distance (GWD) as\n",
    "\n",
    "$$D^f_W(G,G') := W_1(f(G),f(G'))$$\n",
    "\n",
    "The Wasserstein distance is a distance function between probability distributions defined on a given metric space. It is linked to the optimal transport problem, where the aim is to find the most \"inexpensive\" way, in terms of the ground distance, to transport all the probability mass from distribution $\\sigma$ to match distribution $\\mu$. In the context of this work, we deal with finite sets of node embeddings and not with continuous probability distributions. Therefore, the Wasserstein distance can be reformulated as a sum rather than an integral, and uses the matrix notation commonly encountered in the optimal transport literature to represent the transportation plan. Given two sets of vectors $X \\in \\mathbb{R}^{n\\times m}$ and $X' \\in \\mathbb{R}^{n'\\times m}$, the authors equivalently define the Wasserstein distance between them as ([Togninalli et al., 2019](https://arxiv.org/pdf/1906.01277)):\n",
    "\n",
    "$$W_1(X,X') := \\underset{\\{P \\in \\Gamma\\}(X,X')}{\\min} \\langle P, M \\rangle$$</br>\n",
    "\n",
    "Here, $M$ is distance  matrix containing the distances $d(x,x')$ between each element $x$ of $X$ and $x'$ of $X'$, $P \\in \\Gamma$ is a transport matrix (or joing probability), and $\\langle \\dot , \\dot \\rangle$ is the Frobenius dot product. The transport matrix $P$ contains the fractions that indicate how to transport the values from $X$ to $X'$ with the minimal total transport effort. Because we asume that the total mass to be transported equals 1 and is evenly distributed across the elements of $X$ and $X'$, the row an dcolumn values of $P$ must sum up to $\\frac{1}{n}$ and $\\frac{1}{n'}$, respectively. \n",
    "\n",
    "\n",
    "To compute the pairwise Wasserstein Distance of graphs, it is first necessary to calculate the ground distances between each pair of nodes between graphs (e.g., `G1` and `G2`). For categorical node features like the context of this work, [Togninalli et al. (2019)](https://arxiv.org/pdf/1906.01277) suggest using the normalized Hamming distance:\n",
    "\n",
    "$$d_{Ham}(v,v') = \\frac {1}{H+1}\\sum^{H+1}_{i=1} \\rho(v_i,v'_i), \\rho(x,y) = \\begin{cases} 1, x \\ne y \\\\ 0, x = y \\end{cases}$$ \n",
    "\n",
    "The Hamming distance equals 1 when two vectors have no features in common and 0 when the vectors are identical. The Hamming distance is appropriate since the Weisfeiler-Lehman features are categorical, and values carry no meaning. When $d_{Ham}(v,v')$ is iterated through two sets of vectors $X \\in \\mathbb{R}^{n\\times m}$ and $X' \\in \\mathbb{R}^{n'\\times m}$, it gives $M$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b3129-80ef-4a94-a500-1af747c4be52",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Trying with individual building footprints\n",
    "</span>\n",
    "\n",
    "**Comparing G1 and G2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e374df8-9b9d-46c5-8bcd-b6c75db8c33c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare G1 and G2\n",
    "M12 = np.zeros((len(G1_nf_m), len(G2_nf_m)))\n",
    "\n",
    "# define a rho function for 1, x not equal y; and 0, x equals y\n",
    "def rho(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# this is to compute M\n",
    "for index_i, v_i in enumerate(G1_nf_m): # row in graph 1\n",
    "    #print(f\"G1_nf_m:{v_i}\")\n",
    "    for index_i_p, v_i_p in enumerate(G2_nf_m): # row in graph 2\n",
    "        #print(f\"G2_nf_m:{v_i_p}\")\n",
    "        # this part computes dham\n",
    "        ## this part computes the constant 1 / H + 1\n",
    "        constant =  1.0 / (len(v_i)+1) \n",
    "\n",
    "        sum_of_rho = 0\n",
    "        for i in range(len(v_i)): # since begin from 0, so just the original length, no need to +1 (as suggested in the equation)\n",
    "            sum_of_rho += rho(v_i[i],v_i_p[i])\n",
    "            #print(sum_of_rho)\n",
    "   \n",
    "        ## this part times the constant with the summation of the rhos\n",
    "        dham = constant*sum_of_rho # this is dham(v,v'), to compute distance between each vertex (all iterations) of graphs, node1 in X vs node1 in X'\n",
    "\n",
    "        M12[index_i,index_i_p] = dham\n",
    "        \n",
    "M12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c73b7-16f1-4880-a113-4d35071e1b34",
   "metadata": {},
   "source": [
    "From the POT python library, the [`ot.emd2`](https://pythonot.github.io/quickstart.html) function returns the Wasserstein distance. For instance, it can be defined as:\n",
    "\n",
    "`W = ot.emd2(a,b,M)`\n",
    "\n",
    "where `a` and `b` are 1D histograms (sum to 1 and positive) and `M` is the ground cost matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82863fe-64ef-4dc5-8ad0-725c0c4120e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a, the Origin, aka vertices in X (there is n # of vertices), each vertex in G1 should be represented as 1/n and summed up to 1\n",
    "n_G1 = np.full(G1_nf_m.shape[0], 1/ G1_nf_m.shape[0]) # np.full(shape,fill_value) returns a new array of given shape and type, filled with fill_value.\n",
    "n_G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce70010-24e4-4bef-a7f3-7166a36bb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute b, the Destination, aka vertices in X' (there is n # of vertices)\n",
    "n_G2 = np.full(G2_nf_m.shape[0], 1/ G2_nf_m.shape[0])\n",
    "n_G2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a4021-e2e0-429a-852d-ba14728d8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ot\n",
    "# from scipy.stats import wasserstein_distance\n",
    "# import ot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d53f3-39e6-497f-a578-1a2a5df77bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wd_g12 = ot.emd2(n_G1, n_G2, M12)\n",
    "wd_g12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5b0d0-9383-471e-be1a-81e215fd5ea2",
   "metadata": {},
   "source": [
    "**Comparing G1 and G3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159243bf-9f6f-44b8-9b6f-f8c8aef67df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare G1 and G2\n",
    "M13 = np.zeros((len(G1_nf_m), len(G3_nf_m)))\n",
    "\n",
    "# define a rho function for 1, x not equal y; and 0, x equals y\n",
    "def rho(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# this is to compute M\n",
    "for index_i, v_i in enumerate(G1_nf_m): # row in graph 1\n",
    "    #print(f\"G1_nf_m:{v_i}\")\n",
    "    for index_i_p, v_i_p in enumerate(G3_nf_m): # row in graph 2\n",
    "        #print(f\"G2_nf_m:{v_i_p}\")\n",
    "        # this part computes dham\n",
    "        ## this part computes the constant 1 / H + 1\n",
    "        constant =  1.0 / (len(v_i)+1) \n",
    "\n",
    "        sum_of_rho = 0\n",
    "        for i in range(len(v_i)): # since begin from 0, so just the original length, no need to +1 (as suggested in the equation)\n",
    "            sum_of_rho += rho(v_i[i],v_i_p[i])\n",
    "            #print(sum_of_rho)\n",
    "   \n",
    "        ## this part times the constant with the summation of the rhos\n",
    "        dham = constant*sum_of_rho # this is dham(v,v')\n",
    "\n",
    "        M13[index_i,index_i_p] = dham\n",
    "        \n",
    "M13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92641c04-9a1c-46d3-aee4-e1422d234ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a from G1\n",
    "n_G1 = np.full(G1_nf_m.shape[0], 1/ G1_nf_m.shape[0])\n",
    "\n",
    "# compute b from G3\n",
    "n_G3 = np.full(G3_nf_m.shape[0], 1/ G3_nf_m.shape[0])\n",
    "\n",
    "print(f\"a:{n_G1}; \\nb:{n_G3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b240de5-027e-47dd-b155-18de81be852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute wasstersin distance of graphs G1 and G3\n",
    "wd_g13 = ot.emd2(n_G1, n_G3, M13)\n",
    "wd_g13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f035da-ee04-453e-961c-e8bcf48fd783",
   "metadata": {},
   "source": [
    "**Comparing G2 and G3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd668d73-02a5-4efb-9cb2-e2d0c2b2ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare G1 and G2\n",
    "M23 = np.zeros((len(G2_nf_m), len(G3_nf_m)))\n",
    "\n",
    "# define a rho function for 1, x not equal y; and 0, x equals y\n",
    "def rho(x,y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# this is to compute M\n",
    "for index_i, v_i in enumerate(G2_nf_m): # row in graph 1\n",
    "    #print(f\"G1_nf_m:{v_i}\")\n",
    "    for index_i_p, v_i_p in enumerate(G3_nf_m): # row in graph 2\n",
    "        #print(f\"G2_nf_m:{v_i_p}\")\n",
    "        # this part computes dham\n",
    "        ## this part computes the constant 1 / H + 1\n",
    "        constant =  1.0 / (len(v_i)+1) \n",
    "\n",
    "        sum_of_rho = 0\n",
    "        for i in range(len(v_i)): # since begin from 0, so just the original length, no need to +1 (as suggested in the equation)\n",
    "            sum_of_rho += rho(v_i[i],v_i_p[i])\n",
    "            #print(sum_of_rho)\n",
    "   \n",
    "        ## this part times the constant with the summation of the rhos\n",
    "        dham = constant*sum_of_rho # this is dham(v,v')\n",
    "\n",
    "        M23[index_i,index_i_p] = dham\n",
    "        \n",
    "M23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d75820-2e04-4202-a194-53f0d04243b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a from G1\n",
    "n_G2 = np.full(G2_nf_m.shape[0], 1/ G2_nf_m.shape[0])\n",
    "\n",
    "# compute b from G3\n",
    "n_G3 = np.full(G3_nf_m.shape[0], 1/ G3_nf_m.shape[0])\n",
    "\n",
    "print(f\"a:{n_G2}; \\nb:{n_G3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c300eb-63ea-403c-99a1-b2dea24bcdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute wasstersin distance of graphs G2 and G3\n",
    "wd_g23 = ot.emd2(n_G2, n_G3, M23)\n",
    "wd_g23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4b3e0d-1ffe-482f-9907-3feae8f662d8",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Trying with the whole map building footprints\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a3be8-2952-447c-a75c-85af51f66868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Medial axis construction\n",
    "### function #1: Extract poitns from one polygon\n",
    "def pts_to_skeleton(geom):\n",
    "    '''\n",
    "    This function extracts points from a geometry, converts them into a polygon, and draws the skeleton of a geometry. It returns a skeleton graph of a geometry\n",
    "    '''\n",
    "    points = []\n",
    "    for x, y in geom.boundary.coords[:-1]:\n",
    "        points.append(sg.Point2(x,y))\n",
    "    points_to_poly = points[::-1] # make sure it is counter-clockwise\n",
    "    poly_to_skel = sg.Polygon(points_to_poly)\n",
    "    skeleton = sg.skeleton.create_interior_straight_skeleton(poly_to_skel)\n",
    "    return skeleton\n",
    "\n",
    "\n",
    "### function #2: Convert skeleton graphs to networkx graphs\n",
    "def skeleton2networkx(skeleton):\n",
    "    geom_vertices = list(skeleton.vertices)\n",
    "    geom_edges = list(skeleton.halfedges)\n",
    "\n",
    "    # create an empty graph with no nodes and no edges\n",
    "    G=nx.Graph()\n",
    "\n",
    "    # grow undirected (since it adds up from both direction) graph G by adding edge, done by specifying the vertex id at two ends to determine the edge\n",
    "    for item in geom_edges:\n",
    "        if item.is_bisector:\n",
    "            p1 = item.vertex.id # vertex id\n",
    "            p2 = item.opposite.vertex.id\n",
    "            G.add_edge(p1, p2)\n",
    "    #nx.draw(G, with_labels=True)\n",
    "    return G\n",
    "\n",
    "\n",
    "## Weisfeiler-Lehman label propagation\n",
    "### function #3: Weisfeiler-Lehman label propagation\n",
    "### This can be found directly from networkx function, `nx.weisfeiler_lehman_subgraph_hashes()`\n",
    "\n",
    "## Graph embeddings\n",
    "### function #4: # Create node embeddings of graph and convert list of list to matrix for the subsequent computation\n",
    "def graph_embeddings(g_hash):\n",
    "    g_node_features = []\n",
    "    for i in g_hash.values():\n",
    "        g_node_features.append(i)\n",
    "\n",
    "    g_node_features_matrix = np.array(g_node_features)\n",
    "    return g_node_features_matrix\n",
    "\n",
    "## Wasserstein distance computation using optimal transport theory\n",
    "### function #4: rho(x,y) define a rho function for 1, x not equal y; and 0, x equals y\n",
    "def rho(x,y):\n",
    "    '''\n",
    "    This function is part of the dham eq. from Togninalli et al. (2019).\n",
    "    In the nx.weisfeiler_lehman_subgraph_hashes() method, there is an argument to specify iteration(s) of label propagation.\n",
    "    Each vertex is represented by a list of hex code. This function compares one iteration of the hex code in each vertex of X and that of in X'.\n",
    "    '''\n",
    "    if x == y:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "### function #5: Compute the ground cost, M, between graphs\n",
    "def ground_cost_m(graph1, graph2):\n",
    "    '''\n",
    "    This function is translated from the dham eq. from Togninalli et al. (2019)\n",
    "    \n",
    "    INPUT\n",
    "    graph1: matrix of node features of a graph\n",
    "    graph2: matrix of node features of another graph\n",
    "\n",
    "    OUTPUT\n",
    "    An array of distances between vertices in X and X'\n",
    "    '''\n",
    "    # create an empty matrix to store the info later\n",
    "    M12 = np.zeros((len(graph1), len(graph2)))\n",
    "\n",
    "    # compute M\n",
    "    for index_i, v_i in enumerate(graph1): # row in graph 1\n",
    "        for index_i_p, v_i_p in enumerate(graph2): # row in graph 2\n",
    "            # this part computes dham\n",
    "            constant =  1.0 / (len(v_i)+1) # computes the constant 1 / H + 1\n",
    "            # this part computes the sum of rho\n",
    "            sum_of_rho = 0\n",
    "            for i in range(len(v_i)): # since begin from 0, so just the original length, no need to +1 (as suggested in the equation)\n",
    "                sum_of_rho += rho(v_i[i],v_i_p[i])\n",
    "                #print(sum_of_rho)\n",
    "   \n",
    "            ## this part times the constant with the summation of the rhos\n",
    "            dham = constant*sum_of_rho # this is dham(v,v')\n",
    "\n",
    "            M12[index_i,index_i_p] = dham\n",
    "    return M12\n",
    "\n",
    "### function #6: Vectorize a and b for Wasserstein distance computation\n",
    "def od_vector(graph1_nf_m,graph2_nf_m):\n",
    "    '''\n",
    "    Computes a (origin) or b (destination), aka vertices in X (there is n # of vertices), \n",
    "    each vertex in G1 should be represented as 1/n and summed up to 1, \n",
    "    whereas each vertex in G2 should be represented as 1/n' and summed up to 1 as well\n",
    "\n",
    "    INPUT\n",
    "    \n",
    "    '''\n",
    "    n_g1 = np.full(graph1_nf_m.shape[0], 1/ graph1_nf_m.shape[0])\n",
    "    n_g2 = np.full(graph2_nf_m.shape[0], 1/ graph2_nf_m.shape[0])\n",
    "    return n_g1, n_g2\n",
    "\n",
    "\n",
    "## THE BIG FUNCTION - from building.gdf to pairwise Wasserstein Distance\n",
    "def bldgFootprint_pairwiseWassersteinDistance(buildingsgdf):\n",
    "    '''\n",
    "    This function takes building footprints, constructs the medial axis of each footprint, converts them into graph representations, \n",
    "    and computes the pairwise Wasserstein distances between each graph of building footprints\n",
    "    \n",
    "    INPUT\n",
    "    buildingsgdf: Building footprints in gdf format\n",
    "\n",
    "    OUTPUT\n",
    "    Matrix of pairwise Wasserstein distances between graphs of building footprints\n",
    "    '''\n",
    "    # create an empty matrix to store the pairwise Wasserstein distances\n",
    "    pairwiseWD = np.zeros((len(buildingsgdf), len(buildingsgdf)))\n",
    "                          \n",
    "    for i, geom1 in enumerate(buildingsgdf.geometry): \n",
    "        for j, geom2 in enumerate(buildingsgdf.geometry):\n",
    "            # medial axis construction\n",
    "            skeleton1 = pts_to_skeleton(geom1)\n",
    "            skeleton2 = pts_to_skeleton(geom2)\n",
    "            if skeleton1 == None or skeleton2 == None:\n",
    "                #print(i, j)\n",
    "                continue\n",
    "            else:                \n",
    "                # convert skeleton graphs to networkx graphs\n",
    "                g1 = skeleton2networkx(skeleton1)\n",
    "                g2 = skeleton2networkx(skeleton2)\n",
    "                \n",
    "                # Weisfeiler-Lehman label propagation\n",
    "                g_hashes1 = nx.weisfeiler_lehman_subgraph_hashes(g1, iterations=4, digest_size=8)\n",
    "                g_hashes2 = nx.weisfeiler_lehman_subgraph_hashes(g2, iterations=4, digest_size=8)\n",
    "                \n",
    "                #print(g_hashes)\n",
    "                g_embeddings1 = graph_embeddings(g_hashes1)\n",
    "                g_embeddings2 = graph_embeddings(g_hashes2)\n",
    "                #print(f\"G1:{g_embeddings1}; \\nG2:{g_embeddings2}\")\n",
    "\n",
    "                # compute M\n",
    "                m12 = ground_cost_m(g_embeddings1,g_embeddings2)\n",
    "\n",
    "                # vectorize embedded graphs for Wasserstein distance computation\n",
    "                n_g1, n_g2 = od_vector(g_embeddings1,g_embeddings2)\n",
    "\n",
    "                # compute Wasserstein distance\n",
    "                w_ab = ot.emd2(n_g1, n_g2, m12)\n",
    "\n",
    "                pairwiseWD[i,j] = w_ab\n",
    "                \n",
    "    return pairwiseWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60864f3-ac95-411d-a038-7dcd3b50c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test case\n",
    "# ske_a = pts_to_skeleton(buildings_gdf.geometry[20])\n",
    "# gs_a = skeleton2networkx(ske_a)\n",
    "# gs_hashes_a = nx.weisfeiler_lehman_subgraph_hashes(gs_a, iterations=4, digest_size=8)\n",
    "# g_em_a = graph_embeddings(gs_hashes_a)\n",
    "\n",
    "\n",
    "# ske_b = pts_to_skeleton(buildings_gdf.geometry[3])\n",
    "# gs_b = skeleton2networkx(ske_b)\n",
    "# gs_hashes_b = nx.weisfeiler_lehman_subgraph_hashes(gs_b, iterations=4, digest_size=8)\n",
    "# g_em_b = graph_embeddings(gs_hashes_b)\n",
    "# m_ab = ground_cost_m(g_em_a,g_em_b)\n",
    "\n",
    "# n_g1, n_g2 = od_vector(g_em_a,g_em_b)\n",
    "\n",
    "# w_ab = ot.emd2(n_g1, n_g2, m_ab)\n",
    "# w_ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0771fba-2065-423d-87f9-4ddf526f6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case BIG function\n",
    "HV_529E_dist_m = bldgFootprint_pairwiseWassersteinDistance(buildings_gdf)\n",
    "print(HV_529E_dist_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82bd4b-49da-4f1b-8a10-398c145fecdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.max(HV_529E_dist_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310f2e0-4b5f-460f-9c61-718b01b41acc",
   "metadata": {},
   "source": [
    "**2.3 Spectral Clustering**\n",
    "\n",
    "Spectral Clustering is a variant of the clustering algorithm that uses the connectivity between the data points to form the clustering. It uses eigenvalues and eigenvectors of the data matrix to forecast the data into lower dimensions space to cluster the data points. It is based on the idea of a graph representation of data where the data point are represented as nodes and the similarity between the data points are represented by an edge.\n",
    "\n",
    "Here is a summary of steps for Spectral Clustering:\n",
    "\n",
    "1. Build the Adjacency Matrix, which can be done with [Epsilon-neighbourhood Graph, K-Nearest Neighbours, or Fully-Connected Graph](https://www.geeksforgeeks.org/machine-learning/ml-spectral-clustering/)\n",
    "2. Do spectral embedding: Embed data points in a low-dimensional space (*spectral embedding*), in which the clusters are more *obvious*, with the use of the eigenvectors of the graph Laplacian\n",
    "3. Cluster in the New Feature Space: Use k-means to cluster data in the new feature space.\n",
    "4. Visualize the Clusters: Plot the clusters to evaluate the results.\n",
    "\n",
    "According to a [lecture video from a professor from UICU](https://youtu.be/zkgm0i77jQ8?si=F_h7YBL3CQVJLsvC), A **adjacency matrix** is an $n \\times n$ summetric matrix where\n",
    "\n",
    "$$A_{ij} = \\begin{cases} w_{ij}: \\text {weight of edge } (i,j) \\\\ 0: \\text {if no edge between } i,j \\end{cases}$$ \n",
    "\n",
    "Then, we can perform the **Laplacian matrix**:\n",
    "\n",
    "$L = D - A$\n",
    "\n",
    "where $D$ is the diagonal matrix of degrees, $d_i = \\sum\\limits_{\\{j|(i,j)\\in E\\}} w_{ij}$ and $L_{ij} = \\begin{cases} d_{ij}: \\text {if} i = j \\\\ -w_{ij}: \\text {if} (i,j) \\text {is an edge} \\\\ 0: \\text {if no edge between} i,j \\end{cases}$\n",
    "\n",
    "For instance,\n",
    "A = \n",
    "| |1|2|3|4|\n",
    "|:---| :---: | :---: | :---: | :---: |\n",
    "|1|0|$w_{12}$|$w_{13}$|0|\n",
    "|2|$w_{12}$|0|$w_{23}$|$w_{24}$|\n",
    "|3|$w_{13}$|$w_{23}$|0|0|\n",
    "|4|0|$w_{24}$|0|0|\n",
    "\n",
    "L = \n",
    "\n",
    "| |1|2|3|4|\n",
    "|:---| :---: | :---: | :---: | :---: |\n",
    "|1|$d_1$|$-w_{12}$|$-w_{13}$|0|\n",
    "|2|-$w_{12}$|$d_2$|$-w_{23}$|$-w_{24}$|\n",
    "|3|$-w_{13}$|$-w_{23}$|$d_3$|0|\n",
    "|4|0|$-w_{24}$|0|$d_4$|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d1be6-59d4-488e-912a-b98155789e48",
   "metadata": {},
   "source": [
    "**Adjacency matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b2ac-acb2-4fd3-8062-e9ebec85e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct adjacency matrix from the distance matrix\n",
    "# based on the lecture video, in A matrix, it should be 0 in the diagonal\n",
    "# 1 - HV_529E_dist_m will create a diagonal with the value of 1, np.eye() is to subtract the diagonal by 1 to get 0\n",
    "A = 1-HV_529E_dist_m - np.eye(HV_529E_dist_m.shape[0])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e841109-522d-4ee7-ac58-82807d1ec68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct laplacian matrix, L = D - A\n",
    "# compute D\n",
    "D = np.diag(A.sum(axis=1))\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8a4b2-96f6-4ad8-aca2-40dee2ae4558",
   "metadata": {},
   "source": [
    "**Laplacian matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba09b0d-9fd7-469c-ac73-44b0cc8e76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute L\n",
    "L = D - A\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cae0d-32c5-4c1d-939b-861703e7bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find eigenvalues and eigenvectors\n",
    "vals, vecs = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5c2f6-64a0-4330-86de-6d533924f707",
   "metadata": {},
   "source": [
    "**K-mean clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb425c0b-6691-4537-8d99-4997b8c125c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c96f8b-8904-413c-9ebd-20b8ee33c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "clsutering_4 = SpectralClustering(n_clusters=4, affinity='precomputed', assign_labels='kmeans', random_state=0).fit(A)\n",
    "y_pred_4 = clsutering_4.labels_.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3528668-d59e-451d-a899-c5065c99b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4ad9d-277c-4cd8-86cb-6b22166050bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_gdf['cluster_4'] = y_pred_4\n",
    "buildings_gdf['cluster_4'] = buildings_gdf['cluster_4'].astype('category') # Make the column categorical\n",
    "buildings_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7164374-2882-4e90-867a-4a629f975c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buildings_gdf.plot(column='cluster_4', cmap='tab10', legend=True, figsize=(24,24))\n",
    "plt.title(\"Building Clusters (k=4)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5af61e-4147-4385-924b-f55829c77d82",
   "metadata": {},
   "source": [
    "- ***Eigengap heuristic for finding the optimal number of clusters***\n",
    "\n",
    "This paper by [von Luxburg (2007)](https://link.springer.com/content/pdf/10.1007/s11222-007-9033-z.pdf) proposes an approach based on perturbation theory and spectral graph theory to calculate the optimal number of clusters. Eigengap heuristic suggests the number of clusters *k* is usually given by the value of *k* that maximizes the eigengap (difference between consecutive eigenvalues). The larger this eigengap is, the closer the eigenvectors of the ideal case and hence the better spectral clustering works.\n",
    "\n",
    "References for implementing the code below: [Spectral graph clustering and optimal number of clusters estimation](https://medium.com/data-science/spectral-graph-clustering-and-optimal-number-of-clusters-estimation-32704189afbe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320412d3-b548-4999-b9de-cc4a84a501e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csgraph\n",
    "from scipy.sparse.linalg import eigsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e10fa3-d016-43df-968a-24f2f0b801a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigenDecomposition(A, plot = True, topK = 5):\n",
    "    \"\"\"\n",
    "    :param A: Affinity matrix\n",
    "    :param plot: plots the sorted eigen values for visual inspection\n",
    "    :return A tuple containing:\n",
    "    - the optimal number of clusters by eigengap heuristic\n",
    "    - all eigen values\n",
    "    - all eigen vectors\n",
    "    \n",
    "    This method performs the eigen decomposition on a given affinity matrix,\n",
    "    following the steps recommended in the paper:\n",
    "    1. Construct the normalized affinity matrix: L = D‚àí1/2ADÀÜ ‚àí1/2.\n",
    "    2. Find the eigenvalues and their associated eigen vectors\n",
    "    3. Identify the maximum gap which corresponds to the number of clusters\n",
    "    by eigengap heuristic\n",
    "    \n",
    "    References:\n",
    "    https://papers.nips.cc/paper/2619-self-tuning-spectral-clustering.pdf\n",
    "    http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf\n",
    "    \"\"\"\n",
    "    L = csgraph.laplacian(A, normed=True)\n",
    "    n_components = A.shape[0]\n",
    "    \n",
    "    # LM parameter : Eigenvalues with largest magnitude (eigs, eigsh), that is, largest eigenvalues in \n",
    "    # the euclidean norm of complex numbers.\n",
    "#     eigenvalues, eigenvectors = eigsh(L, k=n_components, which=\"LM\", sigma=1.0, maxiter=5000)\n",
    "    eigenvalues, eigenvectors = eigsh(L, k=n_components, which=\"LM\", sigma=1.0, maxiter=5000)\n",
    "    \n",
    "    if plot:\n",
    "        plt.title('Largest eigen values of input matrix')\n",
    "        plt.scatter(np.arange(len(eigenvalues)), eigenvalues)\n",
    "        plt.grid()\n",
    "        \n",
    "    # Identify the optimal number of clusters as the index corresponding\n",
    "    # to the larger gap between eigen values\n",
    "    index_largest_gap = np.argsort(np.diff(eigenvalues))[::-1][:topK]\n",
    "    nb_clusters = index_largest_gap + 1\n",
    "        \n",
    "    return nb_clusters, eigenvalues, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b147d-1249-4fa5-ae4d-9173350244e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k, _, _ = eigenDecomposition(A)\n",
    "print(f\"Optimal number of clusters: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ab63e-d3b5-4eb7-9568-b9a5071d70f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 2\n",
    "clsutering_2 = SpectralClustering(n_clusters=2, affinity='precomputed', assign_labels='kmeans', random_state=0).fit(A)\n",
    "y_pred_2 = clsutering_2.labels_.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01385e8-3106-493e-9a3d-142d68a23201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "buildings_gdf['cluster_2'] = y_pred_2\n",
    "buildings_gdf['cluster_2'] = buildings_gdf['cluster_2'].astype('category') # Make the column categorical\n",
    "buildings_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918d240-f4af-47b3-8519-4dd0791ec794",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_gdf.plot(column='cluster_2', cmap='tab10', legend=True, figsize=(24,24))\n",
    "plt.title(\"Building Clusters (k=2)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf8341-ac72-4ef9-bcdc-9bd8091199c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d63d0-a2e5-4b21-850a-f96fb112ab8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b93f94-2667-4fee-be13-71b3e894a190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9b884-b405-4e9d-beb3-42754396b3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788814b0-3e85-471d-9c97-367dd9826c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8c1ab-94b8-4136-aee4-08cbe04a044f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e54db-0977-4231-8d05-f461038aff96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22d578d8-ac9b-4328-be4b-c556d4a50cf4",
   "metadata": {},
   "source": [
    "## **Userful links**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72968c4b-22e3-4958-aa26-23edf6d50795",
   "metadata": {},
   "source": [
    "**Journal Articles**\n",
    "\n",
    "[1] Duong, S., Rottmann, P., Haunert, J. H., & Mutzel, P. (2023, November). Clustering Building Footprint Polygons Based on Graph Similarity Measures. In *Proceedings of the 1st ACM SIGSPATIAL International Workshop on Advances in Urban-AI* (pp. 22-31). [https://doi.org/10.1145/3615900.3628790](https://doi.org/10.1145/3615900.3628790)</br>\n",
    "[2] Togninalli, M., Ghisu, E., Llinares-L√≥pez, F., Rieck, B., & Borgwardt, K. (2019). Wasserstein weisfeiler-lehman graph kernels. *Advances in neural information processing systems, 32*. [https://doi.org/10.48550/arXiv.1906.01277](https://doi.org/10.48550/arXiv.1906.01277)</br>\n",
    "[3] Von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and computing, 17, 395-416. [https://doi.org/10.1007/s11222-007-9033-z](https://link.springer.com/content/pdf/10.1007/s11222-007-9033-z.pdf)\n",
    "\n",
    "\n",
    "**Medial Axis Construction**\n",
    "\n",
    "[1] [Scikit-geometry documentation](https://scikit-geometry.github.io/scikit-geometry/skeleton.html)</br>\n",
    "[2] [Scikit-geometry GitHub](https://github.com/scikit-geometry/scikit-geometry)</br>\n",
    "[3] [Creating a networkx graph](https://networkx.org/documentation/networkx-1.7/tutorial/tutorial.html)\n",
    "\n",
    "\n",
    "**Weisfeiler-Lehman label propagation**\n",
    "\n",
    "[1] [Graph hashing](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.graph_hashing.weisfeiler_lehman_subgraph_hashes.html)</br>\n",
    "[2] [A lecture video on ML with Graphs](https://youtu.be/buzsHTa4Hgs?si=JmqOYMht5iFzhMqg)\n",
    "\n",
    "\n",
    "**Wasserstein Distance**\n",
    "\n",
    "[1] [POT: Python Optimal Transport (Quick start guide](https://pythonot.github.io/quickstart.html#computing-wasserstein-distance)</br>\n",
    "[2] [POT: Python Optimal Transport (API and modules)](https://pythonot.github.io/all.html#ot.emd2)\n",
    "\n",
    "**Spectral Clustering**\n",
    "\n",
    "[1] [A lecture video on Spectral Clustering](https://youtu.be/zkgm0i77jQ8?si=RnT-WW57sKk1b-00)</br>\n",
    "[2] [GitHub of Spectral Clustering](https://github.com/BSAraujo/Spectral-Clustering) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b5f61-03cf-4eda-8905-86f0b76d2f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
